{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vatsal_28\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==2.1.0+cu117 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1)\n",
      "ERROR: No matching distribution found for torch==2.1.0+cu117\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch==2.1.0+cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(torch_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure PyTorch is installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(torch_device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      "am student, who was also a student at the University of California, Berkeley.\n",
      "\n",
      "\"I was a little bit surprised to see that the student body was so supportive of the student body,\" said the student's mother, who asked not to be\n"
     ]
    }
   ],
   "source": [
    "#Greedy Search for the text generation\n",
    "\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "greedy_output = model.generate(**model_input, max_length=50, do_sample=False)\n",
    "print('Output :')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw back of Greedy is that it hides high probability words bwhind the low probability words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      "i enjoy to do Â a lot of things, but I don't think I've ever done anything like this before. I'm not sure if I've ever done anything like this before. I'm not sure if I\n"
     ]
    }
   ],
   "source": [
    "#Beean Search for the text generation\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "beam_output = model.generate(**model_input, max_new_tokens=40,num_beams = 5,early_stopping=True)\n",
    "\n",
    "print('Output :')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beam search is good at the garmmer and the context of the text generation, but it also repeats the same words as greedy search.\n",
    "To remove the repetition of the words, we give penalty in form of ***n-grams***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      "i am cs student at the University of California, San Diego.\n",
      "\n",
      "\"I've been doing this for a long time,\" he said. \"I've been doing this for a long time. I've been doing\n"
     ]
    }
   ],
   "source": [
    "#Beean Search for the text generation\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "beam_output = model.generate(**model_input, max_new_tokens=40,num_beams = 5,no_repeat_ngram_size=10,early_stopping=True)\n",
    "\n",
    "print('Output :')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the top beams with the generated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      "0:i am cs student at the University of California, San Diego.\n",
      "\n",
      "\"I've been doing this for a long time,\" he said. \"I've been doing this for a long time. I've been doing\n",
      "1:i am cs student at the University of California, San Diego.\n",
      "\n",
      "\"I've been doing this for a long time,\" she said. \"I've been doing this for a long time. I've been doing\n",
      "2:i am cs student at the University of California, San Diego.\n",
      "\n",
      "\"I've been doing this for a long time,\" he said. \"I've been doing this for a long time, and I've been\n",
      "3:i am cs student at the University of California, San Diego.\n",
      "\n",
      "\"I've been doing this for a long time,\" he said. \"I've been doing this for a long time. I've been working\n",
      "4:i am cs student at the University of California, San Diego.\n",
      "\n",
      "\"I've been doing this for a long time,\" he said. \"I've been doing this for a long time, and I've never\n"
     ]
    }
   ],
   "source": [
    "#Beean Search for the text generation\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "beam_output = model.generate(**model_input, max_new_tokens=40,num_beams = 5,no_repeat_ngram_size=10,num_return_sequences=5,early_stopping=True)\n",
    "\n",
    "print('Output :')\n",
    "for i,beam_output in enumerate(beam_output):\n",
    "  print('{}:{}'.format(i,tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving the generation with the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "am student for the station that protagonist Alex Wong posted on a Facebook page in which he called Mather 'a serious hateful racist' ( A New Yorker ).\n",
      "\n",
      "He told The Guardian in a prerecorded telephone\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "sample_output = model.generate(**model_input, max_new_tokens=40,do_sample=True, top_k=0)\n",
    "print(\"Output:\")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "i am cs student for the season. He told me he'd had a hard time getting a job and felt like he'd been left behind because he wasn't able to get a good job.\n",
      "\n",
      "\"Anxiety\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "sample_output = model.generate(**model_input, max_new_tokens=40,do_sample=True, top_k=0,temperature=0.7)\n",
    "print(\"Output:\")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top k sampling ,this is taken by gpt2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "i enjoy walking with my cute dog for the rest of the week.\n",
      "\n",
      "7. Do some research before you decide if you want to buy a pack of shoes\n",
      "\n",
      "If you already have what is listed in each list, it\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "sample_output = model.generate(**model_input, max_new_tokens=40,do_sample=True, top_k=50)\n",
    "print(\"Output:\")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top p sampling (nucleus sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "i dont like coding for you guys that agree but I really feel that it would be good if it are not at all irrational to do it myself :p\n",
      "\n",
      "\n",
      "I think there is not a solitary class member in char\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "sample_output = model.generate(**model_input, max_new_tokens=40,do_sample=True, top_k=0,top_p=0.92)\n",
    "print(\"Output:\")\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0:am student for the Department of Law, at the start of June, and she said she's looking forward to starting her own firm at the same time. \"I have many opportunities to build a company from the\n",
      "1:am student on the faculty. The committee members spoke with Dr. Agha and Dr. R.D. Anam, who both confirmed it would be inappropriate to discuss the details of the disciplinary proceeding.\n",
      "\n",
      "2:am student from San Francisco, I was about to start a graduate program and my classmates did too.\n",
      "\n",
      "We were so happy at the first meeting of our program, after all this was gone, so I\n",
      "3:am student in an international physics school, had died after being struck by lightning in a car parked next to him.\n",
      "\n",
      "\"He was very popular,\" she said. \"When people saw him, they were\n",
      "4:am student who recently found a job in their own world. In some ways, they are less likely to have gone hungry, and they are more likely to have had an accident.\n",
      "\n",
      "A big part of\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "text_input = input('Enter the text: ')\n",
    "model_input = tokenizer(text_input, return_tensors=\"pt\").to(torch_device)\n",
    "sample_output = model.generate(**model_input, max_new_tokens=40,do_sample=True, top_k=50,top_p=0.95,num_return_sequences=5)\n",
    "print(\"Output:\")\n",
    "for i,sample_output in enumerate(sample_output):\n",
    "  print('{}:{}'.format(i,tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
